{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import glob\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"D:\\\\human_fall\\\\dataset\"\n",
    "\n",
    "train_origin_root = 'D:\\\\human_fall\\\\train_origin'\n",
    "train_resize_images = f'{data_root}\\\\train\\\\images'\n",
    "#labels_root = f'{data_root}\\\\train\\\\labels'\n",
    "\n",
    "valid_origin_root = 'D:\\\\human_fall\\\\valid_origin'\n",
    "valid_resize_images = f'{data_root}\\\\val\\\\images'\n",
    "#valid_labels_root = f'{data_root}\\\\valid\\\\labels'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data resize/padding\n",
    "\n",
    "train_root = 'D:\\\\human_fall\\\\dataset\\\\train\\\\images'\n",
    "val_root = 'D:\\\\human_fall\\\\dataset\\\\val\\\\images'\n",
    "test_root = 'D:\\\\human_fall\\\\dataset\\\\test\\\\images'\n",
    "# Create directories if not exist\n",
    "#os.makedirs(train_resize_images, exist_ok=True)\n",
    "#os.makedirs(valid_resize_images, exist_ok=True)\n",
    "\n",
    "# Helper function to resize and pad images\n",
    "def letterbox_image(image, target_size=(640, 640)):\n",
    "    # 원본 이미지 크기\n",
    "    h, w = image.shape[:2]\n",
    "    target_w, target_h = target_size\n",
    "\n",
    "    # 비율 계산\n",
    "    scale = min(target_w / w, target_h / h)\n",
    "\n",
    "    # 새로운 이미지 크기\n",
    "    new_w = int(w * scale)\n",
    "    new_h = int(h * scale)\n",
    "\n",
    "    # 이미지 리사이즈\n",
    "    resized_image = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # 패딩 적용\n",
    "    pad_w = (target_w - new_w) // 2\n",
    "    pad_h = (target_h - new_h) // 2\n",
    "\n",
    "    # 이미지를 타겟 크기로 채워서 새로운 이미지를 만듦\n",
    "    padded_image = cv2.copyMakeBorder(resized_image, pad_h, target_h - new_h - pad_h, pad_w, target_w - new_w - pad_w,\n",
    "                                      cv2.BORDER_CONSTANT, value=[128, 128, 128])\n",
    "\n",
    "    return padded_image\n",
    "\n",
    "# Process Non-Fall Data\n",
    "for file in os.listdir(test_root):\n",
    "    file_path = os.path.join(test_root, file)\n",
    "    image = cv2.imread(file_path)\n",
    "    resized_image = letterbox_image(image)\n",
    "    output_path = os.path.join(test_root, file)\n",
    "    cv2.imwrite(output_path, resized_image)\n",
    "\n",
    "# Process Fall Data\n",
    "#for file in os.listdir(fall_image_path):\n",
    "    #file_path = os.path.join(fall_image_path, file)\n",
    "    #image = cv2.imread(file_path)\n",
    "    #resized_image = letterbox_image(image)\n",
    "    #output_path = os.path.join(output_fall_image_path, file)\n",
    "    #cv2.imwrite(output_path, resized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = 'D:\\\\human_fall\\\\dataset\\\\train\\\\images'\n",
    "val_root = 'D:\\\\human_fall\\\\dataset\\\\val\\\\images'\n",
    "train_labels = 'D:\\\\human_fall\\\\dataset\\\\train\\\\labels'\n",
    "val_labels = 'D:\\\\human_fall\\\\dataset\\\\val\\\\labels'\n",
    "test_root = 'D:\\\\human_fall\\\\dataset\\\\test\\\\images'\n",
    "test_labels = 'D:\\\\human_fall\\\\dataset\\\\test\\\\labels'\n",
    "\n",
    "def update_labels(label_path, original_size, new_size, scale, pad_w, pad_h):\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    new_lines = []\n",
    "    for line in lines:\n",
    "        class_id, x, y, w, h = map(float, line.strip().split())\n",
    "        \n",
    "        # 원본 이미지에서의 픽셀 좌표로 변환\n",
    "        x_pixel = x * original_size[0]\n",
    "        y_pixel = y * original_size[1]\n",
    "        w_pixel = w * original_size[0]\n",
    "        h_pixel = h * original_size[1]\n",
    "        \n",
    "        # 새 이미지에서의 픽셀 좌표로 변환\n",
    "        new_x_pixel = x_pixel * scale + pad_w\n",
    "        new_y_pixel = y_pixel * scale + pad_h\n",
    "        new_w_pixel = w_pixel * scale\n",
    "        new_h_pixel = h_pixel * scale\n",
    "        \n",
    "        # 새 이미지에서의 상대 좌표로 변환\n",
    "        new_x = new_x_pixel / new_size[0]\n",
    "        new_y = new_y_pixel / new_size[1]\n",
    "        new_w = new_w_pixel / new_size[0]\n",
    "        new_h = new_h_pixel / new_size[1]\n",
    "        \n",
    "        new_lines.append(f\"{int(class_id)} {new_x:.6f} {new_y:.6f} {new_w:.6f} {new_h:.6f}\\n\")\n",
    "    \n",
    "    with open(label_path, 'w') as f:\n",
    "        f.writelines(new_lines)\n",
    "\n",
    "# 원본 이미지 크기와 새 이미지 크기\n",
    "original_size = (3840, 2160)\n",
    "new_size = (640, 640)\n",
    "\n",
    "# 스케일 계산\n",
    "scale = min(new_size[0] / original_size[0], new_size[1] / original_size[1])\n",
    "\n",
    "# 패딩 계산\n",
    "new_w = int(original_size[0] * scale)\n",
    "new_h = int(original_size[1] * scale)\n",
    "pad_w = (new_size[0] - new_w) // 2\n",
    "pad_h = (new_size[1] - new_h) // 2\n",
    "\n",
    "# 라벨 파일 업데이트\n",
    "for file in os.listdir(test_labels):\n",
    "    if file.endswith('.txt'):\n",
    "        label_path = os.path.join(test_labels, file)\n",
    "        update_labels(label_path, original_size, new_size, scale, pad_w, pad_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper function to get all patient directories\n",
    "def get_patient_dirs(path):\n",
    "    patient_dirs = {}\n",
    "    for item in os.listdir(path):\n",
    "        patient_id = '_'.join(item.split('_')[:4])  # Get unique identifier\n",
    "        if patient_id not in patient_dirs:\n",
    "            patient_dirs[patient_id] = []\n",
    "        patient_dirs[patient_id].append(os.path.join(path, item))\n",
    "    return list(patient_dirs.values())\n",
    "\n",
    "# Process Non-Fall Data\n",
    "non_fall_patients_video = get_patient_dirs(non_fall_video_path)\n",
    "non_fall_patients_image = get_patient_dirs(non_fall_image_path)\n",
    "\n",
    "for patient_files in non_fall_patients_video:\n",
    "    for file in patient_files:\n",
    "        shutil.copy(file, output_nonfall_video_path)\n",
    "\n",
    "for patient_files in non_fall_patients_image:\n",
    "    for file in patient_files:\n",
    "        shutil.copy(file, output_nonfall_image_path)\n",
    "\n",
    "# Process Fall Data\n",
    "fall_patients_video = []\n",
    "fall_patients_image = []\n",
    "for path in fall_video_paths:\n",
    "    fall_patients_video.extend(get_patient_dirs(path))\n",
    "for path in fall_image_paths:\n",
    "    fall_patients_image.extend(get_patient_dirs(path))\n",
    "\n",
    "selected_patients = random.sample(fall_patients_video, len(fall_patients_video) // 3)\n",
    "\n",
    "for patient_files in selected_patients:\n",
    "    for file in patient_files:\n",
    "        shutil.copy(file, output_fall_video_path)\n",
    "\n",
    "# Ensure image selection matches selected patients\n",
    "selected_patient_ids = {'_'.join(files[0].split('_')[:4]) for files in selected_patients}\n",
    "\n",
    "for patient_files in fall_patients_image:\n",
    "    patient_id = '_'.join(patient_files[0].split('_')[:4])\n",
    "    if patient_id in selected_patient_ids:\n",
    "        for file in patient_files:\n",
    "            shutil.copy(file, output_fall_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train dataset...\n",
      "Processed all JSON files in D:\\human_fall\\dataset\\train\\labels\n",
      "Processing val dataset...\n",
      "Processed all JSON files in D:\\human_fall\\dataset\\val\\labels\n",
      "Processing test dataset...\n",
      "Processed all JSON files in D:\\human_fall\\dataset\\test\\labels\n",
      "All datasets processed.\n"
     ]
    }
   ],
   "source": [
    "# json 파일에 있는 bbox 정보를 yolo 형식으로\n",
    "# class_name x_center y_center w h\n",
    "train_json = 'D:\\\\human_fall\\\\dataset\\\\train\\\\labels'\n",
    "val_json = 'D:\\\\human_fall\\\\dataset\\\\val\\\\labels'\n",
    "test_json = 'D:\\\\human_fall\\\\dataset\\\\test\\\\labels'\n",
    "\n",
    "def convert_bbox(bbox, img_width, img_height):\n",
    "    x1, y1, x2, y2 = map(float, bbox.split(','))\n",
    "    \n",
    "    width = (x2 - x1) / img_width\n",
    "    height = (y2 - y1) / img_height\n",
    "    center_x = (x1 + x2) / (2 * img_width)\n",
    "    center_y = (y1 + y2) / (2 * img_height)\n",
    "    \n",
    "    return center_x, center_y, width, height\n",
    "\n",
    "def determine_class(file_name):\n",
    "    file_name_lower = file_name.lower()\n",
    "    if 'by' in file_name_lower or 'sy' in file_name_lower or 'fy' in file_name_lower:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def process_json(json_path, output_dir, new_width=640, new_height=640):\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    orig_width, orig_height = map(int, data['metadata']['scene_res'].split('x'))\n",
    "    \n",
    "    scale = min(new_width / orig_width, new_height / orig_height)\n",
    "    \n",
    "    scaled_width = int(orig_width * scale)\n",
    "    scaled_height = int(orig_height * scale)\n",
    "    \n",
    "    pad_x = (new_width - scaled_width) // 2\n",
    "    pad_y = (new_height - scaled_height) // 2\n",
    "    \n",
    "    bbox = data['bboxdata']['bbox_location']\n",
    "    x, y, w, h = convert_bbox(bbox, orig_width, orig_height)\n",
    "    \n",
    "    x = (x * scaled_width + pad_x) / new_width\n",
    "    y = (y * scaled_height + pad_y) / new_height\n",
    "    w = (w * scaled_width) / new_width\n",
    "    h = (h * scaled_height) / new_height\n",
    "    \n",
    "    class_id = determine_class(data['metadata']['file_name'])\n",
    "    \n",
    "    result = f\"{class_id} {x:.6f} {y:.6f} {w:.6f} {h:.6f}\"\n",
    "    \n",
    "    output_filename = os.path.splitext(data['metadata']['file_name'])[0] + '.txt'\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(result)\n",
    "\n",
    "def process_all_json_in_folder(input_folder, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith('.json'):\n",
    "            json_path = os.path.join(input_folder, filename)\n",
    "            process_json(json_path, output_folder)\n",
    "    \n",
    "    print(f\"Processed all JSON files in {input_folder}\")\n",
    "\n",
    "# 각 데이터셋 처리\n",
    "datasets = [\n",
    "    ('train', train_json),\n",
    "    ('val', val_json),\n",
    "    ('test', test_json)\n",
    "]\n",
    "\n",
    "for dataset_name, json_folder in datasets:\n",
    "    output_folder = os.path.join(os.path.dirname(json_folder), 'labels_yolo')\n",
    "    print(f\"Processing {dataset_name} dataset...\")\n",
    "    process_all_json_in_folder(json_folder, output_folder)\n",
    "\n",
    "print(\"All datasets processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = \"D:\\\\human_fall\\\\dataset\\\\train\\\\images\"\n",
    "train_label = \"D:\\\\human_fall\\\\dataset\\\\train\\\\labels\"\n",
    "\n",
    "val_root = \"D:\\\\human_fall\\\\dataset\\\\val\\\\images\"\n",
    "val_label = \"D:\\\\human_fall\\\\dataset\\\\val\\\\labels\"\n",
    "\n",
    "test_root = \"D:\\\\human_fall\\\\dataset\\\\test\\\\images\"\n",
    "test_label = \"D:\\\\human_fall\\\\dataset\\\\test\\\\labels\"\n",
    "\n",
    "\n",
    "def clip_coordinates(coord):\n",
    "    return max(0, min(coord, 1))\n",
    "\n",
    "def normalize_coordinates(label_path, image_path):\n",
    "    # 이미지 크기 가져오기\n",
    "    with Image.open(image_path) as img:\n",
    "        img_width, img_height = img.size\n",
    "    \n",
    "    # 라벨 파일 읽기\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    normalized_lines = []\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 5:  # class x y width height 형식 확인\n",
    "            class_id = parts[0]\n",
    "            x = float(parts[1])\n",
    "            y = float(parts[2])\n",
    "            width = float(parts[3])\n",
    "            height = float(parts[4])\n",
    "            \n",
    "            # 좌표 정규화\n",
    "            x_normalized = x / img_width\n",
    "            y_normalized = y / img_height\n",
    "            width_normalized = width / img_width\n",
    "            height_normalized = height / img_height\n",
    "            \n",
    "            x_normalized = clip_coordinates(x_normalized)\n",
    "            y_normalized = clip_coordinates(y_normalized)\n",
    "            width_normalized = clip_coordinates(width_normalized)\n",
    "            height_normalized = clip_coordinates(height_normalized)\n",
    "            \n",
    "            \n",
    "            # 정규화된 좌표로 새 라인 생성\n",
    "            new_line = f\"{class_id} {x_normalized:.6f} {y_normalized:.6f} {width_normalized:.6f} {height_normalized:.6f}\\n\"\n",
    "            normalized_lines.append(new_line)\n",
    "    \n",
    "    # 정규화된 좌표로 파일 다시 쓰기\n",
    "    with open(label_path, 'w') as f:\n",
    "        f.writelines(normalized_lines)\n",
    "\n",
    "image_folder = val_root\n",
    "label_folder = val_label\n",
    "\n",
    "for filename in os.listdir(label_folder):\n",
    "    if filename.endswith('.txt'):\n",
    "        label_path = os.path.join(label_folder, filename)\n",
    "        image_path = os.path.join(image_folder, filename.replace('.txt', '.jpg'))\n",
    "        \n",
    "        if os.path.exists(image_path):\n",
    "            normalize_coordinates(label_path, image_path)\n",
    "        else:\n",
    "            print(f\"Image not found for label: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용되지 않는 캐시 메모리 삭제\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 경로에 yaml파일 생성 : D:\\human_fall\\dataset\n"
     ]
    }
   ],
   "source": [
    "data_root = 'D:\\\\human_fall\\\\dataset'\n",
    "train_root = f'{data_root}\\\\train\\\\images'\n",
    "val_root = f'{data_root}\\\\val\\\\images'\n",
    "class_names = {0 : 'Non_Fall', 1 : 'Fall'}\n",
    "num_classes = len(class_names)\n",
    "\n",
    "yaml_info = {\n",
    "    'path' : data_root,\n",
    "    'names': class_names,\n",
    "    'nc': num_classes,\n",
    "    'train': train_root,\n",
    "    'val': val_root\n",
    "}\n",
    "\n",
    "with open('yaml_info_yolov8s.yaml', 'w') as f : \n",
    "    yaml.dump(yaml_info, f)\n",
    "print(f'이 경로에 yaml파일 생성 : {data_root}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.17 available  Update with 'pip install -U ultralytics'\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=D:\\project\\prjvenv\\yaml_info_yolov8s.yaml, epochs=50, time=None, patience=30, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cuda, workers=20, project=None, name=human_fall_s29, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\human_fall_s29\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116822  ultralytics.nn.modules.head.Detect           [2, [128, 256, 512]]          \n",
      "Model summary: 225 layers, 11,136,374 parameters, 11,136,358 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\human_fall_s29', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\human_fall\\dataset\\train\\labels.cache... 87730 images, 0 backgrounds, 0 corrupt: 100%|██████████| 87730/87730 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\human_fall\\dataset\\val\\labels.cache... 42294 images, 0 backgrounds, 0 corrupt: 100%|██████████| 42294/42294 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\human_fall_s29\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 20 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\human_fall_s29\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50      4.23G          0      12.01          0          0        640:  46%|████▌     | 2535/5484 [10:03<11:41,  4.20it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov8s.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mproject\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mprjvenv\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43myaml_info_yolov8s.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhuman_fall_s\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      7\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\engine\\model.py:803\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    800\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[1;32m--> 803\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\engine\\trainer.py:207\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\engine\\trainer.py:385\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp):\n\u001b[0;32m    384\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_batch(batch)\n\u001b[1;32m--> 385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    387\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m world_size\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\nn\\tasks.py:107\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;124;03mPerform forward pass of the model for either training or inference.\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    (torch.Tensor): Loss if x is a dict (training), or network predictions (inference).\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\nn\\tasks.py:289\u001b[0m, in \u001b[0;36mBaseModel.loss\u001b[1;34m(self, batch, preds)\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_criterion()\n\u001b[0;32m    288\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m preds\n\u001b[1;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\utils\\loss.py:220\u001b[0m, in \u001b[0;36mv8DetectionLoss.__call__\u001b[1;34m(self, preds, batch)\u001b[0m\n\u001b[0;32m    218\u001b[0m dtype \u001b[38;5;241m=\u001b[39m pred_scores\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    219\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m pred_scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 220\u001b[0m imgsz \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# image size (h,w)\u001b[39;00m\n\u001b[0;32m    221\u001b[0m anchor_points, stride_tensor \u001b[38;5;241m=\u001b[39m make_anchors(feats, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m    223\u001b[0m \u001b[38;5;66;03m# Targets\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model = YOLO('yolov8s.pt')\n",
    "result = model.train(data = 'D:\\\\project\\\\prjvenv\\\\yaml_info_yolov8s.yaml', epochs = 50, batch = 16, imgsz =640, device = device, workers = 20, amp = True, patience = 30, name = 'human_fall_s')\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"실행 시간: {execution_time:.4f} 초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

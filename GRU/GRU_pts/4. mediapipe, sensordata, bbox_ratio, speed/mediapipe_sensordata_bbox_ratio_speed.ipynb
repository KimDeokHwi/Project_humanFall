{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mediapipe의 랜드마크, bbox의 좌표, bbox의 ratio, speed를 학습시켰을 때의 비디오 테스트\n",
    "* input_size = 28\n",
    "* sequence_length = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MediaPipe Pose 초기화\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜드마크 인덱스 정의 \n",
    "LANDMARKS = [0, 11, 12, 15, 16, 23, 24, 25, 26, 27, 28]\n",
    "\n",
    "# GRU 모델 정의\n",
    "class FallDetectionGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, num_classes=3, dropout=0.5):\n",
    "        super(FallDetectionGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_head_upper_body_speed(current_frame, prev_frame):\n",
    "    h = np.array([current_frame['landmark_0']['x'], current_frame['landmark_0']['y']])\n",
    "    l = np.array([current_frame['landmark_11']['x'], current_frame['landmark_11']['y']])\n",
    "    r = np.array([current_frame['landmark_12']['x'], current_frame['landmark_12']['y']])\n",
    "    \n",
    "    prev_h = np.array([prev_frame['landmark_0']['x'], prev_frame['landmark_0']['y']])\n",
    "    prev_l = np.array([prev_frame['landmark_11']['x'], prev_frame['landmark_11']['y']])\n",
    "    prev_r = np.array([prev_frame['landmark_12']['x'], prev_frame['landmark_12']['y']])\n",
    "    \n",
    "    center_new = (h + l + r) / 3\n",
    "    center_prev = (prev_h + prev_l + prev_r) / 3\n",
    "    \n",
    "    return distance.euclidean(center_new, center_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_draw_bbox(frame, landmarks):\n",
    "    x_coordinates = landmarks[:, 0]\n",
    "    y_coordinates = landmarks[:, 1]\n",
    "    \n",
    "    x1 = max(0, int(np.min(x_coordinates)))\n",
    "    y1 = max(0, int(np.min(y_coordinates)))\n",
    "    x2 = min(frame.shape[1], int(np.max(x_coordinates)))\n",
    "    y2 = min(frame.shape[0], int(np.max(y_coordinates)))\n",
    "\n",
    "    # 바운딩 박스를 조금 더 넓게 조정 (각 방향으로 패딩 추가)\n",
    "    padding = 50\n",
    "    x1 = max(0, x1 - padding)\n",
    "    y1 = max(0, y1 - padding)\n",
    "    x2 = min(frame.shape[1], x2 + padding)\n",
    "    y2 = min(frame.shape[0], y2 + padding)\n",
    "\n",
    "    # 바운딩 박스 비율 계산\n",
    "    bbox_width = x2 - x1\n",
    "    bbox_height = y2 - y1\n",
    "    bbox_ratio = bbox_width / bbox_height if bbox_height != 0 else float('inf')  # 높이가 0일 경우 무한대로 설정\n",
    "    \n",
    "    # 바운딩 박스 그리기\n",
    "    cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "\n",
    "    return (x1, y1), (x2, y2), bbox_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for FallDetectionGRU:\n\tsize mismatch for gru.weight_ih_l0: copying a param with shape torch.Size([192, 27]) from checkpoint, the shape in current model is torch.Size([192, 28]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m FallDetectionGRU(input_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mproject\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mprjvenv\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mGRU\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mGRU_pts\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m4. mediapipe, sensordata, bbox_ratio, speed\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mmediapipe_sensordata_bbox_ratio_speed_except_normalizaion.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     14\u001b[0m data_sequence \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for FallDetectionGRU:\n\tsize mismatch for gru.weight_ih_l0: copying a param with shape torch.Size([192, 27]) from checkpoint, the shape in current model is torch.Size([192, 28])."
     ]
    }
   ],
   "source": [
    "# 비디오 파일 경로 설정 및 열기\n",
    "video_path = 'D:\\\\human_fall\\\\re_video\\\\validation\\\\Y\\\\00170_H_A_SY_C5.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "sequence_length = 3\n",
    "\n",
    "# GRU 모델 초기화 및 가중치 로드\n",
    "input_size = 28  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FallDetectionGRU(input_size).to(device)\n",
    "model.load_state_dict(torch.load('D:\\\\project\\\\prjvenv\\\\GRU\\\\GRU_pts\\\\4. mediapipe, sensordata, bbox_ratio, speed\\\\mediapipe_sensordata_bbox_ratio_speed_except_normalizaion.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "data_sequence = []\n",
    "previous_landmarks_dict = None\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # BGR 이미지를 RGB로 변환 및 랜드마크 추출\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        landmarks_dict_current = {f'landmark_{i}': {'x': results.pose_landmarks.landmark[i].x,\n",
    "                                                     'y': results.pose_landmarks.landmark[i].y} for i in range(len(results.pose_landmarks.landmark))}\n",
    "        \n",
    "        landmarks_array_current_flattened = []\n",
    "        \n",
    "        for landmark_idx in LANDMARKS:\n",
    "            landmark = results.pose_landmarks.landmark[landmark_idx]\n",
    "            landmarks_array_current_flattened.append([landmark.x * frame.shape[1], landmark.y * frame.shape[0]])  # 픽셀 좌표로 변환\n",
    "            \n",
    "            # 랜드마크를 비디오 프레임에 표시\n",
    "            cv2.circle(frame, (int(landmark.x * frame.shape[1]), int(landmark.y * frame.shape[0])), 5, (0, 255, 0), -1)\n",
    "\n",
    "        # 현재 프레임의 랜드마크 배열 변환 \n",
    "        landmarks_array_current_flattened_np = np.array(landmarks_array_current_flattened)\n",
    "\n",
    "        # 바운딩 박스 및 비율 계산\n",
    "        bbox_ratio_value = calculate_and_draw_bbox(frame, landmarks_array_current_flattened_np)\n",
    "\n",
    "        # 속도 정보 추가\n",
    "        speed_value = 0.0\n",
    "        \n",
    "        if previous_landmarks_dict is not None:\n",
    "            speed_value = calculate_head_upper_body_speed(landmarks_dict_current, previous_landmarks_dict)\n",
    "\n",
    "        previous_landmarks_dict = landmarks_dict_current  \n",
    "\n",
    "        # 랜드마크 + bbox 정보 추가\n",
    "        landmarks_array_combined = np.concatenate((landmarks_array_current_flattened_np.flatten(),\n",
    "                                                   [bbox_ratio_value],\n",
    "                                                   [speed_value]))\n",
    "\n",
    "        data_sequence.append(landmarks_array_combined)\n",
    "\n",
    "        if len(data_sequence) == sequence_length:\n",
    "            input_data = np.array(data_sequence).reshape(1, sequence_length, -1)  \n",
    "            input_tensor = torch.FloatTensor(input_data).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_tensor)\n",
    "                predicted_label_id = torch.argmax(outputs).item()\n",
    "\n",
    "                # 예측된 클래스 이름 출력\n",
    "                label_name_mapping = {0: 'Normal', 1: 'Danger', 2: 'Fall'}\n",
    "                predicted_label_name = label_name_mapping[predicted_label_id]\n",
    "\n",
    "                print(f\"Predicted Class: {predicted_label_name}\")  \n",
    "\n",
    "                # 예측된 클래스 이름을 바운딩 박스 왼쪽 상단에 표시\n",
    "                cv2.putText(frame,\n",
    "                            predicted_label_name,\n",
    "                            (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            1.5,(255,255,255),3)\n",
    "\n",
    "            data_sequence.pop(0)  \n",
    "\n",
    "        # 랜드마크 표시 \n",
    "        mp_drawing.draw_landmarks(frame, results.pose_landmarks)\n",
    "\n",
    "    resized_frame = cv2.resize(frame,(1920 ,1080))\n",
    "    \n",
    "    # 비디오 프레임 출력 \n",
    "    cv2.imshow('Fall Detection', resized_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "         break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

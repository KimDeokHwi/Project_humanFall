{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mediapipe의 랜드마크만 학습시켰을 때의 비디오 테스트\n",
    "* input_size = 27\n",
    "* sequence_length = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FallDetectionGRU(\n",
       "  (gru): GRU(22, 64, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (fc): Linear(in_features=64, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 랜드마크 인덱스 정의 \n",
    "LANDMARKS = [0, 11, 12, 15, 16, 23, 24, 25, 26, 27, 28]\n",
    "\n",
    "class FallDetectionGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, output_size=3, dropout=0.5):\n",
    "        super(FallDetectionGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# 모델 초기화 및 가중치 로드\n",
    "input_size = 22 \n",
    "model = FallDetectionGRU(input_size).to(device)\n",
    "model.load_state_dict(torch.load('D:\\\\project\\\\prjvenv\\\\GRU\\\\GRU_pts\\\\2. mediapipe & sensordata\\\\mediapipe_sensordata_except_normalization.pt', map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'D:\\\\human_fall\\\\re_video\\\\validation\\\\Y\\\\00170_H_A_SY_C4.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "sequence_length = 3  # 시퀀스 길이 설정 (훈련 시 사용한 값과 일치해야 함)\n",
    "data_sequence = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # BGR 이미지를 RGB로 변환 및 랜드마크 추출\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        landmarks = []\n",
    "        \n",
    "        # 랜드마크 추출\n",
    "        for landmark_idx in LANDMARKS:\n",
    "            landmark = results.pose_landmarks.landmark[landmark_idx]\n",
    "            landmarks.append([landmark.x, landmark.y])\n",
    "        \n",
    "        # 랜드마크 배열 변환 및 시퀀스 추가\n",
    "        landmarks_array = np.array(landmarks).flatten()\n",
    "        \n",
    "        if len(data_sequence) < sequence_length:\n",
    "            data_sequence.append(landmarks_array)\n",
    "        \n",
    "        if len(data_sequence) == sequence_length:\n",
    "            input_data = np.array(data_sequence).reshape(1, sequence_length, -1)\n",
    "            input_tensor = torch.FloatTensor(input_data).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_tensor)\n",
    "                predicted_label_id = torch.argmax(outputs).item()\n",
    "\n",
    "                # 예측된 클래스 이름 출력\n",
    "                label_name = {0: 'Normal', 1: 'Danger', 2: 'Fall'}\n",
    "                predicted_label_name = label_name[predicted_label_id]\n",
    "                if predicted_label_name == 'Normal' : \n",
    "                    cv2.putText(frame, f\"Class: {predicted_label_name}\", (50, 150), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            5, (0, 255, 0), 4)\n",
    "                elif predicted_label_name == 'Danger' :\n",
    "                    cv2.putText(frame, f\"Class: {predicted_label_name}\", (50, 150), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            5, (0, 255, 255), 4)\n",
    "                else : \n",
    "                    cv2.putText(frame, f\"Class: {predicted_label_name}\", (50, 150), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            5, (0, 0, 255), 4)\n",
    "            \n",
    "            # 시퀀스 초기화 (이전 시퀀스를 제거하고 새로운 시퀀스를 시작할 수 있음)\n",
    "            data_sequence.pop(0)  # 첫 번째 프레임 제거\n",
    "    \n",
    "    resized_frame = cv2.resize(frame, (1920, 1080))\n",
    "    # 비디오 프레임 출력\n",
    "    cv2.imshow('Fall Detection', resized_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 리소스 해제\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
